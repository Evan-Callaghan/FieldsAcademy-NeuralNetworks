{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdb2dc3-653b-42bd-9a46-ab7c7fcf5f81",
   "metadata": {},
   "source": [
    "## Evaluating all data imputation techniques\n",
    "\n",
    "#### Considered methods:\n",
    "- mean imputation\n",
    "- Last observation carried forward imputation\n",
    "- linear imputation\n",
    "- k-nearest neighbours\n",
    "- CNN based imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad00ec44-0f21-4645-a170-5d7766e9b5b9",
   "metadata": {},
   "source": [
    "#### 1. Configuring setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf952afd-afa3-4405-9e43-9500d6e56d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tensorflow tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8972522-c402-4b56-a2a1-84ebcba5fc89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6924c1-b2e3-417e-a9e0-daabf0d47905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Reading the traffic data-frames\n",
    "traffic_data = pd.read_csv('Output_Data/traffic_data.csv').drop(columns = ['Hour'])\n",
    "traffic_data_5 = pd.read_csv('Output_Data/traffic_data_mapped_5.csv').drop(columns = ['Hour'])\n",
    "traffic_data_15 = pd.read_csv('Output_Data/traffic_data_mapped_15.csv').drop(columns = ['Hour'])\n",
    "traffic_data_25 = pd.read_csv('Output_Data/traffic_data_mapped_25.csv').drop(columns = ['Hour'])\n",
    "hour = pd.read_csv('Output_Data/traffic_data.csv')['Hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a324c23f-70be-4101-85fd-b40384468b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Subsetting the data-frame to only account for the testing data\n",
    "def creating_test(data):\n",
    "    data_1 = data.iloc[38400:38600, 0:200]\n",
    "    data_2 = data.iloc[38400:38600, 50:250]; data_2.columns = data_1.columns\n",
    "    data_3 = data.iloc[38400:38600, 100:300]; data_3.columns = data_1.columns\n",
    "    data_4 = data.iloc[38400:38600, 150:350]; data_4.columns = data_1.columns\n",
    "    data_5 = data.iloc[38400:38600, 200:400]; data_5.columns = data_1.columns\n",
    "    data_6 = data.iloc[38700:38900, 0:200]; data_6.columns = data_1.columns\n",
    "    data_7 = data.iloc[38700:38900, 50:250]; data_7.columns = data_1.columns\n",
    "    data_8 = data.iloc[38700:38900, 100:300]; data_8.columns = data_1.columns\n",
    "    data_9 = data.iloc[38700:38900, 150:350]; data_9.columns = data_1.columns\n",
    "    data_10 = data.iloc[38700:38900, 200:400]; data_10.columns = data_1.columns\n",
    "    \n",
    "    return pd.concat([data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10], axis = 0, ignore_index = True)\n",
    "\n",
    "test = creating_test(traffic_data)\n",
    "test_5 = creating_test(traffic_data_5)\n",
    "test_15 = creating_test(traffic_data_15)\n",
    "test_25 = creating_test(traffic_data_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f0e53f-547e-42f2-828d-15e724293220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Saving the original dimensions\n",
    "n = test.shape[0]; p = test.shape[1]\n",
    "\n",
    "## Scaling the values to be between 0 and 1\n",
    "test = test / 255.\n",
    "test_5 = test_5 / 255.\n",
    "test_15 = test_15 / 255.\n",
    "test_25 = test_25 / 255.\n",
    "\n",
    "## Flattening the data-frames for easier computation\n",
    "test_flat = np.array(test).flatten()\n",
    "test_5_flat = np.array(test_5).flatten()\n",
    "test_15_flat = np.array(test_15).flatten()\n",
    "test_25_flat = np.array(test_25).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c60a02-9eec-43ea-92f6-35398ff6b9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Recording the indices where each data-frame has missing values\n",
    "missing_idx_5 = np.argwhere(test_5_flat == 1.0)\n",
    "missing_idx_15 = np.argwhere(test_15_flat == 1.0)\n",
    "missing_idx_25 = np.argwhere(test_25_flat == 1.0)\n",
    "\n",
    "## Extracting the true values for these indices\n",
    "Y_true_5 = test_flat[missing_idx_5]\n",
    "Y_true_15 = test_flat[missing_idx_15]\n",
    "Y_true_25 = test_flat[missing_idx_25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aeb0820-bb4c-4a0e-9973-d675e1720b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining MSE and MAE functions\n",
    "def imputed_mse(Y_true, Y_pred):\n",
    "    return mean_squared_error(Y_true, Y_pred, squared = False)\n",
    "\n",
    "def imputed_mae(Y_true, Y_pred):\n",
    "    return mean_absolute_error(Y_true, Y_pred)\n",
    "\n",
    "def evaluation(Y_true, Y_pred):\n",
    "    MSE = imputed_mse(Y_true, Y_pred); MAE = imputed_mae(Y_true, Y_pred)\n",
    "    return MSE, MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37918ef-24cd-4c59-999e-5953d8dee8d1",
   "metadata": {},
   "source": [
    "#### 2. Mean Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b227e1b-8ce1-4e69-af16-f3f7bf109aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_imputer(col):\n",
    "    col = np.where(col == 1.0, np.mean(col), col)\n",
    "    return col\n",
    "\n",
    "mean_imputed_5 = np.array(test_5.copy().apply(mean_imputer, axis = 0)).flatten()\n",
    "mean_imputed_15 = np.array(test_15.copy().apply(mean_imputer, axis = 0)).flatten()\n",
    "mean_imputed_25 = np.array(test_25.copy().apply(mean_imputer, axis = 0)).flatten()\n",
    "\n",
    "mean_imputed_5 = mean_imputed_5[missing_idx_5]\n",
    "mean_imputed_15 = mean_imputed_15[missing_idx_15]\n",
    "mean_imputed_25 = mean_imputed_25[missing_idx_25]\n",
    "\n",
    "mean_imputer_mse_5, mean_imputer_mae_5 = evaluation(Y_true_5, mean_imputed_5)\n",
    "mean_imputer_mse_15, mean_imputer_mae_15 = evaluation(Y_true_15, mean_imputed_15)\n",
    "mean_imputer_mse_25, mean_imputer_mae_25 = evaluation(Y_true_25, mean_imputed_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37cffd35-56c7-42aa-8b48-3daa42267e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07323306937038288 0.06267482610738885\n",
      "0.1904966550508372 0.18340013290942975\n",
      "0.37528951894229434 0.37093955306196313\n"
     ]
    }
   ],
   "source": [
    "print(mean_imputer_mse_5, mean_imputer_mae_5)\n",
    "print(mean_imputer_mse_15, mean_imputer_mae_15)\n",
    "print(mean_imputer_mse_25, mean_imputer_mae_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5964273-dd2b-4d3e-a0aa-2a1c5575097e",
   "metadata": {},
   "source": [
    "#### 3. Last Observation Carried Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a63aeaa2-e0e9-4487-91b8-bfddb273cdec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def locf_imputer(data, missing_idx):\n",
    "    \n",
    "    for idx in missing_idx:\n",
    "        i = 1\n",
    "        while np.isnan(data[idx-i]):\n",
    "            i += 1\n",
    "        data[idx] = data[idx-i]\n",
    "    return data\n",
    "\n",
    "locf_imputed_5 = locf_imputer(test_5_flat, missing_idx_5)[missing_idx_5]\n",
    "locf_imputed_15 = locf_imputer(test_15_flat, missing_idx_15)[missing_idx_15]\n",
    "locf_imputed_25 = locf_imputer(test_25_flat, missing_idx_25)[missing_idx_25]\n",
    "\n",
    "locf_imputer_mse_5, locf_imputer_mae_5 = evaluation(Y_true_5, locf_imputed_5)\n",
    "locf_imputer_mse_15, locf_imputer_mae_15 = evaluation(Y_true_15, locf_imputed_15)\n",
    "locf_imputer_mse_25, locf_imputer_mae_25 = evaluation(Y_true_25, locf_imputed_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada3dda8-8522-42a5-9392-12f100e67dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04809841142147701 0.028607450289030476\n",
      "0.04889635259149022 0.028722079757440802\n",
      "0.04996210766678489 0.028991109543814245\n"
     ]
    }
   ],
   "source": [
    "print(locf_imputer_mse_5, locf_imputer_mae_5)\n",
    "print(locf_imputer_mse_15, locf_imputer_mae_15)\n",
    "print(locf_imputer_mse_25, locf_imputer_mae_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e30fe1-d713-45d2-bd7a-fbe38cc62411",
   "metadata": {},
   "source": [
    "#### 4. Linear imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a00cd5a4-426a-4293-9f8e-767e047220a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_imputer(data, missing_idx):\n",
    "    \n",
    "    for idx in missing_idx:\n",
    "        a = 1\n",
    "        while np.isnan(data[idx-a]):\n",
    "            a += 1\n",
    "        if idx < 1999:\n",
    "            b = 1\n",
    "            while np.isnan(data[idx+b]):\n",
    "                b += 1\n",
    "        else: b = -1\n",
    "        data[idx] = (data[idx-a] + data[idx+b]) / 2\n",
    "    return data\n",
    "\n",
    "linear_imputed_5 = linear_imputer(test_5_flat, missing_idx_5)[missing_idx_5]\n",
    "linear_imputed_15 = linear_imputer(test_15_flat, missing_idx_15)[missing_idx_15]\n",
    "linear_imputed_25 = linear_imputer(test_25_flat, missing_idx_25)[missing_idx_25]\n",
    "\n",
    "linear_imputer_mse_5, linear_imputer_mae_5 = evaluation(Y_true_5, linear_imputed_5)\n",
    "linear_imputer_mse_15, linear_imputer_mae_15 = evaluation(Y_true_15, linear_imputed_15)\n",
    "linear_imputer_mse_25, linear_imputer_mae_25 = evaluation(Y_true_25, linear_imputed_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a889bfa1-2e0b-4611-b4ca-d8dad96f0da2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04809404130623528 0.028601608612834593\n",
      "0.04888810757855967 0.028715928254446986\n",
      "0.04995207058453482 0.02898364163798717\n"
     ]
    }
   ],
   "source": [
    "print(linear_imputer_mse_5, linear_imputer_mae_5)\n",
    "print(linear_imputer_mse_15, linear_imputer_mae_15)\n",
    "print(linear_imputer_mse_25, linear_imputer_mae_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5735ef9-89f7-4fc7-8f73-14e007cc64be",
   "metadata": {},
   "source": [
    "#### 5. K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c061f9d3-9127-4c39-9430-8d7d7dbe644e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def knn_imputer(data):\n",
    "    return KNNImputer(n_neighbors = 5).fit_transform(data)\n",
    "\n",
    "knn_test_5 = pd.DataFrame(np.where(test_5 == 1.0, np.nan, test_5))\n",
    "knn_test_15 = pd.DataFrame(np.where(test_15 == 1.0, np.nan, test_15))\n",
    "knn_test_25 = pd.DataFrame(np.where(test_25 == 1.0, np.nan, test_25))\n",
    "\n",
    "knn_imputed_5 = np.array(knn_imputer(knn_test_5)).flatten()\n",
    "knn_imputed_15 = np.array(knn_imputer(knn_test_15)).flatten()\n",
    "knn_imputed_25 = np.array(knn_imputer(knn_test_25)).flatten()\n",
    "\n",
    "knn_imputed_5 = knn_imputed_5[missing_idx_5]\n",
    "knn_imputed_15 = knn_imputed_15[missing_idx_15]\n",
    "knn_imputed_25 = knn_imputed_25[missing_idx_25]\n",
    "\n",
    "knn_imputer_mse_5, knn_imputer_mae_5 = evaluation(Y_true_5, knn_imputed_5)\n",
    "knn_imputer_mse_15, knn_imputer_mae_15 = evaluation(Y_true_15, knn_imputed_15)\n",
    "knn_imputer_mse_25, knn_imputer_mae_25 = evaluation(Y_true_25, knn_imputed_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0963085e-3e3a-4836-97ff-6565e592becd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016465334677439974 0.007034292489167325\n",
      "0.016426942770228416 0.007178376626211829\n",
      "0.017240141503471737 0.007726060209106818\n"
     ]
    }
   ],
   "source": [
    "print(knn_imputer_mse_5, knn_imputer_mae_5)\n",
    "print(knn_imputer_mse_15, knn_imputer_mae_15)\n",
    "print(knn_imputer_mse_25, knn_imputer_mae_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2f9e9-4c65-403b-a743-21354e9cc10c",
   "metadata": {},
   "source": [
    "#### CNN based imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f6535d5-784d-419a-a484-995947e917f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 files belonging to 1 classes.\n",
      "Found 10 files belonging to 1 classes.\n",
      "Found 10 files belonging to 1 classes.\n",
      "Found 10 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "## Reading the testing image data sets\n",
    "testing_5 = tf.keras.utils.image_dataset_from_directory('Image_Data/Testing/Missing/five', labels = None, color_mode = 'grayscale', \n",
    "                                                      batch_size = None, seed = 42, image_size = (200, 200), shuffle = False)\n",
    "testing_15 = tf.keras.utils.image_dataset_from_directory('Image_Data/Testing/Missing/fifteen', labels = None, color_mode = 'grayscale', \n",
    "                                                      batch_size = None, seed = 42, image_size = (200, 200), shuffle = False)\n",
    "testing_25 = tf.keras.utils.image_dataset_from_directory('Image_Data/Testing/Missing/twenty-five', labels = None, color_mode = 'grayscale', \n",
    "                                                      batch_size = None, seed = 42, image_size = (200, 200), shuffle = False)\n",
    "testing_target = tf.keras.utils.image_dataset_from_directory('Image_Data/Testing/Filled', labels = None, color_mode = 'grayscale', \n",
    "                                                      batch_size = None, seed = 42, image_size = (200, 200), shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "298583f9-f594-4ddc-a56f-78f7758cfa93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Scaling the data to be between 0 and 1\n",
    "testing_5 = testing_5.map(lambda x:(x/255))\n",
    "testing_15 = testing_15.map(lambda x:(x/255))\n",
    "testing_25 = testing_25.map(lambda x:(x/255))\n",
    "testing_target = testing_target.map(lambda x:(x/255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0bc3d73-434b-40c7-a5c6-30f8f0756934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Converting tf.Dataset objects to numpy arrays\n",
    "def dataset_to_numpy(ds):\n",
    "    images = []\n",
    "    for i, image in enumerate(tfds.as_numpy(ds)): \n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "X_test_5 = dataset_to_numpy(testing_5)\n",
    "X_test_15 = dataset_to_numpy(testing_15)\n",
    "X_test_25 = dataset_to_numpy(testing_25)\n",
    "Y_test = dataset_to_numpy(testing_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3888b43d-65f0-4d7b-b07b-69f6d46a3a41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "## Loading the CNN models\n",
    "autoencoder_5 = load_model(os.path.join('Models', 'autoencoder_missing_5.h5'))\n",
    "autoencoder_15 = load_model(os.path.join('Models', 'autoencoder_missing_15.h5'))\n",
    "autoencoder_25 = load_model(os.path.join('Models', 'autoencoder_missing_25.h5'))\n",
    "\n",
    "## Predicting on the test sets\n",
    "testing_preds_5 = autoencoder_5.predict(X_test_5)\n",
    "testing_preds_15 = autoencoder_15.predict(X_test_15)\n",
    "testing_preds_25 = autoencoder_25.predict(X_test_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcd28259-4ba4-45f8-9a3a-cbebbfa448f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining MSE and MAE functions specifically for CNN predictions\n",
    "def imputed_mse(Y_true, Y_pred):\n",
    "    return mean_squared_error(Y_true, Y_pred, squared = False)\n",
    "\n",
    "def imputed_mae(Y_true, Y_pred):\n",
    "    return mean_absolute_error(Y_true, Y_pred)\n",
    "\n",
    "def evaluation(X, Y, preds):\n",
    "    mse = list(); mae = list()\n",
    "    \n",
    "    for i in range(len(preds)):\n",
    "        \n",
    "        ## Flattening the arrays \n",
    "        X_flat = np.squeeze(X[i]).flatten()\n",
    "        Y_flat = np.squeeze(Y[i]).flatten()\n",
    "        preds_flat = np.squeeze(preds[i]).flatten()\n",
    "        \n",
    "        ## Finding the index of imputed values\n",
    "        missing_idx = np.argwhere(X_flat == 1.0)\n",
    "        \n",
    "        ## Extracting the Y_true and Y_pred values\n",
    "        Y_true = Y_flat[missing_idx]\n",
    "        Y_pred = preds_flat[missing_idx]\n",
    "        \n",
    "        ## Computing the metrics:\n",
    "        mse.append(imputed_mse(Y_true, Y_pred))\n",
    "        mae.append(imputed_mae(Y_true, Y_pred))\n",
    "        \n",
    "    return mse, mae\n",
    "\n",
    "CNN_imputer_mse_5, CNN_imputer_mae_5 = evaluation(X_test_5, Y_test, testing_preds_5)\n",
    "CNN_imputer_mse_15, CNN_imputer_mae_15 = evaluation(X_test_15, Y_test, testing_preds_15)\n",
    "CNN_imputer_mse_25, CNN_imputer_mae_25 = evaluation(X_test_25, Y_test, testing_preds_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c042e541-0b87-411c-8774-516b6fcb373b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05831934 0.041224577\n",
      "0.05640576 0.04095196\n",
      "0.04808302 0.031264655\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(CNN_imputer_mse_5), np.mean(CNN_imputer_mae_5))\n",
    "print(np.mean(CNN_imputer_mse_15), np.mean(CNN_imputer_mae_15))\n",
    "print(np.mean(CNN_imputer_mse_25), np.mean(CNN_imputer_mae_25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
