{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca3f801-99ad-47b4-a311-c4997e6e80fd",
   "metadata": {},
   "source": [
    "## Generating Traffic Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420b3c6-7c61-465f-9ee5-970af1886e5a",
   "metadata": {},
   "source": [
    "#### 1. Configuring setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64b30f-285e-43c4-a1f6-8c0514151c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14325033-5b4a-465a-8402-45e54685e570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = Path(\"Image_Data/Training/Filled/\"); train_dir.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "train_missing_dir_5 = Path(\"Image_Data/Training/Missing/five/\"); train_missing_dir_5.mkdir(parents = True, exist_ok = True)\n",
    "train_missing_dir_15 = Path(\"Image_Data/Training/Missing/fifteen/\"); train_missing_dir_15.mkdir(parents = True, exist_ok = True)\n",
    "train_missing_dir_25 = Path(\"Image_Data/Training/Missing/twenty-five/\"); train_missing_dir_25.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "test_dir = Path(\"Image_Data/Testing/Filled/\"); test_dir.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "test_missing_dir_5 = Path(\"Image_Data/Testing/Missing/five/\"); test_missing_dir_5.mkdir(parents = True, exist_ok = True)\n",
    "test_missing_dir_15 = Path(\"Image_Data/Testing/Missing/fifteen/\"); test_missing_dir_15.mkdir(parents = True, exist_ok = True)\n",
    "test_missing_dir_25 = Path(\"Image_Data/Testing/Missing/twenty-five/\"); test_missing_dir_25.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "pd.set_option('display.max_columns', None, 'display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229367bf-99ae-4105-9d61-19cc79f1b159",
   "metadata": {},
   "source": [
    "#### 2. Constructing the traffic observation data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457e58c-d979-424d-a419-0be24e1dbb12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Initializing traffic_data data-frame\n",
    "traffic_data = pd.DataFrame()\n",
    "\n",
    "## Concatenating each data file to traffic_data\n",
    "for year in os.listdir('Traffic_Data/'):\n",
    "    for file_name in os.listdir(os.path.join('Traffic_Data', year)):\n",
    "        file = pd.read_csv(os.path.join('Traffic_Data', year, file_name))\n",
    "        traffic_data = pd.concat([traffic_data, file], axis = 0)\n",
    "        \n",
    "## Changing 'Hour' variable to datetime and sorting by date\n",
    "traffic_data['Hour'] = pd.to_datetime(traffic_data['Hour'])\n",
    "traffic_data = traffic_data.sort_values('Hour').reset_index(drop = True)\n",
    "\n",
    "## Reading California I-5 sensor information\n",
    "all_sensors = pd.read_csv('Misc_Data/pems_sensors.csv'); all_sensors['ID'] = all_sensors['ID'].astype(str)\n",
    "\n",
    "## Extracting columns from traffic_data data-frame\n",
    "data_sensors = pd.DataFrame({'Sensor': traffic_data.drop(columns = ['Hour', '# Lane Points', '% Observed']).columns})\n",
    "data_sensors['ID'] = data_sensors['Sensor'].str.split('-', n = 1, expand = True)[0].astype(str)\n",
    "\n",
    "## Inner joining data-frames and sorting by ordering (S --> N, San Diego --> Oregon)\n",
    "new_sensors = data_sensors.merge(all_sensors, how = 'inner', on = 'ID').sort_values('Ordering').reset_index(drop = True)\n",
    "\n",
    "## Extracting proper column ordering and adding 'Hour' to the beginning\n",
    "all_columns = new_sensors['Sensor'].tolist()\n",
    "all_columns.insert(0, 'Hour')\n",
    "\n",
    "## Changing column ordering of traffic_data\n",
    "traffic_data = traffic_data[all_columns]\n",
    "\n",
    "## Dropping all columns in traffic_data with missing values\n",
    "traffic_data = traffic_data.dropna(axis = 'columns')\n",
    "\n",
    "## Cutting some rows to make n_cols % 10 = 0\n",
    "traffic_data = traffic_data.iloc[:, 0:461]\n",
    "\n",
    "## Cutting some rows to make n_rows % 200 = 0\n",
    "traffic_data = traffic_data.iloc[0:40000,]\n",
    "\n",
    "## Saving 'Hour' variable\n",
    "hour = traffic_data['Hour']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8257c-1313-4330-a64c-1838d6438baa",
   "metadata": {},
   "source": [
    "#### 3. Data preprocessing and saving data-frames to 'Output' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f3033-804a-4a59-b691-24117b0a4e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Removing 'Hour' variable for scaling\n",
    "traffic_data = traffic_data.drop(columns = ['Hour'])\n",
    "\n",
    "## Flattening the data for quicker operations\n",
    "flat_data = traffic_data.to_numpy().flatten().astype(int)\n",
    "\n",
    "## Storing the min, max, and dimensions\n",
    "minimum = np.min(flat_data); maximum = np.max(flat_data)\n",
    "n = traffic_data.shape[0]; p = traffic_data.shape[1]\n",
    "\n",
    "## Scaling data to be between 0-254\n",
    "flat_data = 254 * (flat_data - minimum) / (maximum - minimum)\n",
    "\n",
    "## Reshaping the array back to its original shape\n",
    "scaled_data = pd.DataFrame(flat_data.reshape((n, p)), columns = traffic_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4564b742-1d50-45f1-b205-0d78a26ed088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining function to randomly remove a given proportion of observations\n",
    "def delete(col, frac):\n",
    "    col.loc[col.sample(frac = frac).index] = np.nan\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b45d47-161e-4cd9-85ed-785ac815d487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## traffic_data - complete and scaled data-frame\n",
    "traffic_data = scaled_data.copy()\n",
    "pd.concat([hour, traffic_data], axis = 1).to_csv('Output_Data/traffic_data.csv', index = False)\n",
    "\n",
    "## Creating data-frames with missing values\n",
    "traffic_data_missing_5 = scaled_data.apply(delete, axis = 0, frac = 0.05)\n",
    "traffic_data_missing_15 = scaled_data.apply(delete, axis = 0, frac = 0.15)\n",
    "traffic_data_missing_25 = scaled_data.apply(delete, axis = 0, frac = 0.25)\n",
    "\n",
    "## traffic_data_mapped_5 - all missing values mapped to 255\n",
    "traffic_data_mapped_5 = traffic_data_missing_5.fillna(value = 255)\n",
    "pd.concat([hour, traffic_data_mapped_5], axis = 1).to_csv('Output_Data/traffic_data_mapped_5.csv', index = False)\n",
    "\n",
    "## traffic_data_mapped_15 - all missing values mapped to 255\n",
    "traffic_data_mapped_15 = traffic_data_missing_15.fillna(value = 255)\n",
    "pd.concat([hour, traffic_data_mapped_15], axis = 1).to_csv('Output_Data/traffic_data_mapped_15.csv', index = False)\n",
    "\n",
    "## traffic_data_mapped_25 - all missing values mapped to 255\n",
    "traffic_data_mapped_25 = traffic_data_missing_25.fillna(value = 255)\n",
    "pd.concat([hour, traffic_data_mapped_25], axis = 1).to_csv('Output_Data/traffic_data_mapped_25.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b7b9a-fdbb-4570-a356-d7884a448cc4",
   "metadata": {},
   "source": [
    "#### 4. Generating training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641e25b-8735-4c21-aac2-9005f65aa6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to generate images and store to disk\n",
    "def generate_training_images(df, image_dir, idx_start, idx_end):\n",
    "    \n",
    "    ## Initializing list to store images\n",
    "    images = list()\n",
    "\n",
    "    ## Initializing data windows\n",
    "    n_time_windows = 192; n_sensor_windows = 27\n",
    "    \n",
    "    ## Subsetting the data-frame\n",
    "    df = df.iloc[idx_start:idx_end,]\n",
    "\n",
    "    for i in range(0, n_time_windows):\n",
    "        for j in range(0, n_sensor_windows):\n",
    "\n",
    "            ## Locking in data windows\n",
    "            time_loc = [(i * 200), (i * 200 + 200)]; sensor_loc = [(j * 10), (j * 10 + 200)]\n",
    "\n",
    "            ## Creating img with window subset\n",
    "            img = np.array(df.iloc[time_loc[0]:time_loc[1], sensor_loc[0]:sensor_loc[1]])\n",
    "\n",
    "            ## Appending to lists\n",
    "            images.append(img)\n",
    "    \n",
    "    ## Sanity check\n",
    "    print(\"Loaded traffic data:\")\n",
    "    print(f\" - np.shape(images) {np.shape(images)}\")\n",
    "    \n",
    "    ## Storing images to disk\n",
    "    store_many_disk(images, image_dir)\n",
    "    \n",
    "def store_many_disk(images, image_dir):\n",
    "    \n",
    "    ## Saving images one by one\n",
    "    for i, image in enumerate(images):\n",
    "        storing = Image.fromarray(image)\n",
    "        storing = storing.convert('L')\n",
    "        storing.save(image_dir / f\"{i}.png\")\n",
    "    print(f'-- {i+1} images saved to disk -- ')\n",
    "    \n",
    "## Defining indices for training set\n",
    "train_idx_start = 0; train_idx_end = 38400\n",
    "\n",
    "## Storing training images to disk\n",
    "generate_training_images(traffic_data, train_dir, train_idx_start, train_idx_end)\n",
    "generate_training_images(traffic_data_mapped_5, train_missing_dir_5, train_idx_start, train_idx_end)\n",
    "generate_training_images(traffic_data_mapped_15, train_missing_dir_15, train_idx_start, train_idx_end)\n",
    "generate_training_images(traffic_data_mapped_25, train_missing_dir_25, train_idx_start, train_idx_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdd68c-53e2-4646-823a-7f39d4c8a0e4",
   "metadata": {},
   "source": [
    "#### 5. Generating testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c447e5-9ae5-4862-bd12-a70843f527bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to generate images and store to disk\n",
    "def generate_testing_images(df, image_dir, idx_start, idx_end):\n",
    "    \n",
    "    ## Initializing list to store images\n",
    "    images = list()\n",
    "\n",
    "    ## Initializing data windows\n",
    "    n_time_windows = 2; n_sensor_windows = 5\n",
    "    \n",
    "    ## Subsetting the data-frame\n",
    "    df = df.iloc[idx_start:idx_end,].reset_index(drop = True)\n",
    "\n",
    "    for i in range(0, n_time_windows):\n",
    "        for j in range(0, n_sensor_windows):\n",
    "\n",
    "            ## Locking in data windows\n",
    "            time_loc = [(i * 300), (i * 300 + 200)]; sensor_loc = [(j * 50), (j * 50 + 200)]\n",
    "\n",
    "            ## Creating img with window subset\n",
    "            img = np.array(df.iloc[time_loc[0]:time_loc[1], sensor_loc[0]:sensor_loc[1]])\n",
    "            \n",
    "            ## Appending to lists\n",
    "            images.append(img)\n",
    "    \n",
    "    ## Sanity check\n",
    "    print(\"Loaded traffic data:\")\n",
    "    print(f\" - np.shape(images) {np.shape(images)}\")\n",
    "    \n",
    "    ## Storing images to disk\n",
    "    store_many_disk(images, image_dir)\n",
    "    \n",
    "def store_many_disk(images, image_dir):\n",
    "    \n",
    "    ## Saving images one by one\n",
    "    for i, image in enumerate(images):\n",
    "        storing = Image.fromarray(image)\n",
    "        storing = storing.convert('L')\n",
    "        storing.save(image_dir / f\"{i}.png\")\n",
    "    print(f'-- {i+1} images saved to disk -- ')\n",
    "    \n",
    "## Defining indices for testing set\n",
    "test_idx_start = 38400; test_idx_end = 40000\n",
    "\n",
    "## Storing training images to disk\n",
    "generate_testing_images(traffic_data, test_dir, test_idx_start, test_idx_end)\n",
    "generate_testing_images(traffic_data_mapped_5, test_missing_dir_5, test_idx_start, test_idx_end)\n",
    "generate_testing_images(traffic_data_mapped_15, test_missing_dir_15, test_idx_start, test_idx_end)\n",
    "generate_testing_images(traffic_data_mapped_25, test_missing_dir_25, test_idx_start, test_idx_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d3000-87d2-48d9-80cf-7fa024d452e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 6. Sanity Check: Dislaying a few sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df5061-f15f-44f9-bcd6-e7b9e091afb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to read n_images from disk\n",
    "def read_many_disk(n_images):\n",
    "    images = []\n",
    "    for image_id in range(n_images):\n",
    "        images.append(np.array(Image.open(train_dir / f\"{image_id}.png\")))\n",
    "    return images\n",
    "\n",
    "## Reading in a samplle image\n",
    "n_images = 100\n",
    "imgs = read_many_disk(n_images)\n",
    "\n",
    "## Plotting an example training image\n",
    "plt.imshow(imgs[99], cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea1258-726f-46b5-9fb5-df7364a12df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to read n_images from disk\n",
    "def read_many_disk(n_images):\n",
    "    images = []\n",
    "    for image_id in range(n_images):\n",
    "        images.append(np.array(Image.open(train_missing_dir_5 / f\"{image_id}.png\")))\n",
    "    return images\n",
    "\n",
    "## Reading in a samplle image\n",
    "n_images = 100\n",
    "imgs = read_many_disk(n_images)\n",
    "\n",
    "## Plotting an example training image\n",
    "plt.imshow(imgs[99], cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88849eca-1dc5-4dc9-871a-4cd68d9ebba8",
   "metadata": {},
   "source": [
    "#### All images using the traffic data are now stored on disk and ready to be used by the encoder-decoder model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
